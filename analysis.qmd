---
title: Analysis
description: Here we provide a detailed analysis using more sophisticated statistics techniques.
toc: true
draft: false
---

![](https://upload.wikimedia.org/wikipedia/commons/7/77/Pebbleswithquarzite.jpg)

This comes from the file `analysis.qmd`.

We describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You'll also reflect on next steps and further analysis.

The audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc. 

While the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points.
If you want you can link back to your blog posts or create separate pages with more details.

The style of this paper should aim to be that of an academic paper. 
I don't expect this to be of publication quality but you should keep that aim in mind.
Avoid using "we" too frequently, for example "We also found that ...". Describe your methodology and your findings but don't describe your whole process.



## Note on Attribution

In general, you should try to provide links to relevant resources, especially those that helped you. You don't have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don't need a formal citation.

If you are directly quoting from a source, please make that clear. You can show quotes using `>` like this

```         
> To be or not to be.
```

> To be or not to be.

------------------------------------------------------------------------

## Rubric: On this page

You will

-   Introduce what motivates your Data Analysis (DA)
    -   Which variables and relationships are you most interested in?
    -   What questions are you interested in answering?
    -   Provide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.
-   Modeling and Inference
    -   The page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.
    -   Explain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)
    -   Describe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.
-   Explain the flaws and limitations of your analysis
    -   Are there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?
-   Clarity Figures
    -   Are your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?
    -   Each figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)
    -   Default `lm` output and plots are typically not acceptable.
-   Clarity of Explanations
    -   How well do you explain each figure/result?
    -   Do you provide interpretations that suggest further analysis or explanations for observed phenomenon?
-   Organization and cleanliness.
    -   Make sure to remove excessive warnings, hide most or all code, organize with sections or multiple pages, use bullets, etc.
    -   This page should be self-contained, i.e. provide a description of the relevant data.
    
-----------------------------------------------------------------------------

## Loading Clean Data

```{r}
library(here)
# Set the path to the RDS file
cleaned_dataset_path_rds <- here("dataset", "cleaned_NYSERDA_LMI_Census_2013-2015.rds")

# Load the dataset from the RDS file
clean_data <- readRDS(cleaned_dataset_path_rds)

head(clean_data)
```

## Analysis

**For Blog Post 4:**

```{r}
library(ggplot2)
library(reshape2)

# Creating a cross-tabulation of Race / Ethnicity and Education Level
ct <- table(clean_data$"Race...Ethnicity", clean_data$"Education.Level")

# Melting the table for ggplot
ct_melted <- melt(ct)

# Plotting with counts in each box
ggplot(ct_melted, aes(x = Var2, y = Var1, fill = value)) + 
  geom_tile() +  
  geom_text(aes(label = value), color = "white", size = 4) +  # Showing the count
  scale_fill_gradient(low = "lightblue", high = "blue") + 
  labs(x = "Education Level", y = "Race / Ethnicity", fill = "Count", 
       title = "Heatmap of Education Level by Race / Ethnicity") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
```{r}
# Same as before but with percentages of each education level
ct_percent <- prop.table(ct, 2) * 100
ct_percent_melted <- melt(ct_percent)

# Plotting with percentage in each box
ggplot(ct_percent_melted, aes(x = Var2, y = Var1, fill = value)) + 
  geom_tile() +  
  geom_text(aes(label = sprintf("%.1f%%", value)), color = "white", size = 4) +  # showing percentage
  scale_fill_gradient(low = "lightblue", high = "blue") +  
  labs(x = "Education Level", y = "Race / Ethnicity", fill = "Percentage", 
       title = "Heatmap of Education Level by Race / Ethnicity (Percentage)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
```{r}
library(dplyr)
library(forcats)

# Adding a numeric representation for income groups
clean_data$Income.Numeric <- case_when(
  clean_data$Income.Groups == "$0 to <$10,000" ~ 5,
  clean_data$Income.Groups == "$10,000-<$20,000" ~ 15,
  clean_data$Income.Groups == "$20,000-<$30,000" ~ 25,
  clean_data$Income.Groups == "$30,000-<$40,000" ~ 35,
  clean_data$Income.Groups == "$40,000-<$50,000" ~ 45,
  clean_data$Income.Groups == "$50,000+" ~ 55,
  TRUE ~ NA_real_
)

# Creating the plot
ggplot(clean_data, aes(x = fct_reorder(Education.Level, Income.Numeric, .fun = mean), y = Income.Numeric, fill = Race...Ethnicity)) +
  geom_bar(stat = "summary", fun = "mean", position = position_dodge()) +
  theme_minimal() +
  labs(
    x = "Education Level",
    y = "Average Income",
    fill = "Race / Ethnicity",
    title = "Average Income by Education Level and Race / Ethnicity"
  ) +
  theme(
    axis.text.x = element_text(angle = 55, hjust = 1),
    legend.position = "top",
    panel.grid.major.y = element_line(color = "black", size = 0.5),
    panel.grid.minor.y = element_line(color = "black", size = 0.5)
  ) +
  scale_fill_brewer(palette = "Set3")

```


## Loading Clean Data

```{r}
library(here)
# Set the path to the RDS file
cleaned_dataset_path_rds <- here("dataset", "cleaned_NYSERDA_LMI_Census_2013-2015.rds")

# Load the dataset from the RDS file
clean_data <- readRDS(cleaned_dataset_path_rds)

head(clean_data)
```

## Analysis

For Blog Post 4:

```{r}
library(ggplot2)
library(reshape2)

# Creating a cross-tabulation of Race / Ethnicity and Education Level
ct <- table(clean_data$`Race_Ethnicity`, clean_data$`Education_Level`)

# Melting the table for ggplot
ct_melted <- melt(ct)

# Plotting with counts in each box
ggplot(ct_melted, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +  
  geom_text(aes(label = value), color = "white", size = 4) +  # Showing the count
  scale_fill_gradient(low = "lightblue", high = "blue") +
  labs(x = "Education Level", y = "Race / Ethnicity", fill = "Count",
       title = "Heatmap of Education Level by Race / Ethnicity") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
```{r}
# Same as before but with percentages of each education level
ct_percent <- prop.table(ct, 2) * 100
ct_percent_melted <- melt(ct_percent)

# Plotting with percentage in each box
ggplot(ct_percent_melted, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +  
  geom_text(aes(label = sprintf("%.1f%%", value)), color = "white", size = 4) +  # showing percentage
  scale_fill_gradient(low = "lightblue", high = "blue") +  
  labs(x = "Education Level", y = "Race / Ethnicity", fill = "Percentage",
       title = "Heatmap of Education Level by Race / Ethnicity (Percentage)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
```{r}
library(dplyr)
library(forcats)

clean_data$`Income Numeric` <- case_when(
  clean_data$`Income_Groups` == "$0 to <$10,000" ~ 5,
  clean_data$`Income_Groups` == "$10,000-<$20,000" ~ 15,
  clean_data$`Income_Groups` == "$20,000-<$30,000" ~ 25,
  clean_data$`Income_Groups` == "$30,000-<$40,000" ~ 35,
  clean_data$`Income_Groups` == "$40,000-<$50,000" ~ 45,
  clean_data$`Income_Groups` == "$50,000+" ~ 55,
  TRUE ~ NA_real_
)

ggplot(clean_data, aes(x = fct_reorder(`Education_Level`, `Income Numeric`, .fun=mean), y = `Income Numeric`, fill = `Race_Ethnicity`)) +
  geom_bar(stat = "summary", fun = "mean", position = position_dodge()) +
  theme_minimal() +
  labs(x = "Education Level", y = "Average Income", fill = "Race / Ethnicity", title = "Average Income by Education Level and Race / Ethnicity") +
  theme(axis.text.x = element_text(angle = 55, hjust = 1),
        legend.position = "top",
        panel.grid.major.y = element_line(color = "black", size = 0.5),
        panel.grid.minor.y = element_line(color = "black", size = 0.5)) +
  scale_fill_brewer(palette = "Set3")


```


----------------------------------------------------------------------------------------
## New Analysis

# Libraries

```{r}
library(ggplot2)
library(dplyr)
library(readr)
library(here)
library(caret)
library(tidyverse)  # For data manipulation
library(modelr)     # For weighted regression

```

```{r}
# Load the dataset
clean_data <- readRDS(here("dataset", "cleaned_NYSERDA_LMI_Census_2013-2015.rds"))
```

1. Analyzing income distribution by race/ethnicity

```{r}
# Option 1

# Calculating weighted proportions
grouped_data <- clean_data %>%
  group_by(Race = `Race_Ethnicity`, Income = `Income_Groups`) %>%
  summarise(Weighted_Count = sum(`Household_Weight`), .groups = 'drop')

total_weights_per_race <- grouped_data %>%
  group_by(Race) %>%
  summarise(Total_Weight = sum(Weighted_Count), .groups = 'drop')

grouped_data <- grouped_data %>%
  left_join(total_weights_per_race, by = "Race") %>%
  mutate(Proportion = Weighted_Count / Total_Weight)

# stacked bar plot
ggplot(grouped_data, aes(x = Race, y = Proportion, fill = Income)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Weighted Distribution of Income Groups by Race/Ethnicity",
       x = "Race / Ethnicity",
       y = "Proportion") +
  scale_fill_brewer(palette = "Paired")

```
```{r}
# Option 2

# Calculating weighted proportions
plot_data <- clean_data %>%
  group_by(`Race_Ethnicity`, `Income_Groups`) %>%
  summarise(Weighted_Count = sum(`Household_Weight`), .groups = 'drop') %>%
  group_by(`Race_Ethnicity`) %>%
  mutate(Total_Weight = sum(Weighted_Count)) %>%
  mutate(Proportion = Weighted_Count / Total_Weight)

# Creating weighted histograms
ggplot(plot_data, aes(x = `Income_Groups`, y = Proportion, fill = `Income_Groups`)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~`Race_Ethnicity`, scales = "free_x") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Income Distribution by Race/Ethnicity",
       x = "Income Groups",
       y = "Weighted Proportion") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

2. Analyzing Housing Characteristics by Race/Ethnicity:

```{r}
weighted_counts <- clean_data %>%
  group_by(`Race_Ethnicity`, `Owner_Renter_Status`) %>%
  summarise(Weighted.Count = sum(`Household_Weight`), .groups = 'drop')

# Plot
ggplot(weighted_counts, aes(x=`Race_Ethnicity`, y=Weighted.Count, fill=`Owner_Renter_Status`)) +
  geom_bar(stat="identity", position=position_dodge()) +
  labs(title="Weighted Housing Status Distribution by Race / Ethnicity",
       x="Race / Ethnicity", y="Weighted Count") +
  scale_y_continuous(labels = scales::comma) +
  scale_fill_brewer(palette="Set1") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
3. Analyzing Impact of Race/Ethnicity on Educational Outcomes:

```{r}
weighted_education_counts <- clean_data %>%
  group_by(`Race_Ethnicity`, `Education_Level`) %>%
  summarise(Weighted.Count = sum(`Household_Weight`), .groups = 'drop')

# Plot
ggplot(weighted_education_counts, aes(x=`Race_Ethnicity`, y=Weighted.Count, fill=`Education_Level`)) +
  geom_bar(stat="identity", position=position_dodge()) +
  labs(title="Weighted Education Level Distribution by Race / Ethnicity",
       x="Race / Ethnicity", y="Weighted Count") +
  scale_y_continuous(labels = scales::comma) +  
  scale_fill_viridis_d() +  
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(legend.position="right")
```

4. Geographical Map

```{r}
library(sf)
ny_counties <- st_read("dataset-ignore/tl_2023_us_county.shp")
demographic_data <- clean_data  

ny_counties <- ny_counties[ny_counties$STATEFP == '36', ]

#head(ny_counties)
```


```{r}
#install.packages("modeest")
library(modeest)

# Loading geographic data
ny_counties_sf <- st_as_sf(ny_counties)

# Definitions for mapping income to numeric values
income_levels <- c("$0 to <$10,000" = 5000,
                   "$10,000-<$20,000" = 15000,
                   "$20,000-<$30,000" = 25000,
                   "$30,000-<$40,000" = 35000,
                   "$40,000-<$50,000" = 45000,
                   "$50,000+" = 75000)

# Adding a numeric income column to the demographic data
demographic_data <- demographic_data %>%
  mutate(Income_Numeric = income_levels[as.character(Income_Groups)])

# Aggregating demographic data to find least common race and average income
demographic_data_aggregated <- demographic_data %>%
  group_by(County) %>%
  summarise(
    Least_Common_Race_Ethnicity = names(sort(table(Race_Ethnicity), decreasing = FALSE))[1],
    Average_Income = weighted.mean(Income_Numeric, Household_Weight, na.rm = TRUE),
    .groups = 'drop'
  )

# Merging
merged_data <- ny_counties_sf %>%
  left_join(demographic_data_aggregated, by = c("NAME" = "County"))
```

Static Maps 

```{r}
library(scales) 

#Color palettes for the maps
colors <- scale_fill_brewer(palette = "Pastel1", na.value = "grey50", guide = "legend")
income_colors <- scale_fill_viridis_c(
  name = "Average Income",
  labels = dollar_format(prefix="$"),
  na.value = "grey50"
)

# Plotting map for Least Common Race/Ethnicity by County
race_map <- ggplot(data = merged_data) +
  geom_sf(aes(fill = Least_Common_Race_Ethnicity), color = "black", size = 0.25) +
  colors +
  labs(
    title = "Least Common Race/Ethnicity by County",
    fill = "Race/Ethnicity"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

# Plotting map for Average Income by County
income_map <- ggplot(data = merged_data) +
  geom_sf(aes(fill = Average_Income), color = "black", size = 0.25) +
  income_colors +
  labs(
    title = "Average Income by County in New York",
    fill = "Average Income"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom",
        legend.text = element_text(angle = 45, hjust = 1),
        legend.key.size = unit(1.5, "lines")) 

print(race_map)
print(income_map)
```





## Statistical Modelling

**For Blog post 7:**
Model 1

```{r}

```




